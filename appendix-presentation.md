# Appendix: Focus Group Presentation Materials

*This document contains the core presentation materials used across all five stakeholder focus groups. The content includes (anonymized) discussion scenarios and (provocative) statements, as well as questions designed to elicit stakeholder value perspectives. Parts of the materials were adapted for each stakeholder group while maintaining consistent core scenarios.*

## Introduction
### Context [ANONYMIZED]

### Stakeholder Mapping
Participants were shown the group focus and their position in the stakeholder ecosystem.

### Values to Consider
Participants were shown and reminded of the set of values as described in the respective section in [`appendix-q&a.md`](appendix-q&a.md#4-possible-values-to-consider-in-recsys).

## Discussion and Scenarios
*Note: These scenarios were presented as simplified conceptual approaches (with mockups) to facilitate stakeholder discussion about underlying value priorities, rather than specific algorithmic implementations.*

### Topic 1: Visibility and Representation

**Key Question**: How can recommendations balance promoting popular items with ensuring visibility for diverse collections?

#### Scenario Options Presented:
**Scenario A: Engagement-Based Recommendations**
- Recommendations prioritize documents with demonstrated user interest and interaction patterns
- Emphasizes content that has proven valuable to previous users within the scholarly community

**Scenario B: Representation-Based Recommendations**  
- Recommendations ensure balanced visibility across different archives, collections, and content characteristics
- Helps users discover materials beyond those with established research attention

**Scenario C: Personalization-Based Recommendations**
- Recommendations prioritize documents with relevance to individual user research contexts and interests
- Emphasizes contextual appropriateness and research workflow integration

#### Discussion Questions (All Groups):
1. Which approach (popularity, diversity, utility) would best serve your needs as [stakeholder] and why?
2. From your perspective as [stakeholder], how would you measure "fair representation" of different archives?
3. How would you measure usefulness?

#### Provocative Statements by Stakeholder Group:

**Upstream Stakeholders:**
- *"I'm more concerned that my highest-quality items receive visibility than ensuring equal representation of all my collection materials."*
- *"Archives with smaller or less digitized collections deserve more visibility in recommendations to counter existing imbalance in digital representation."*

**Provider Stakeholders:**
- *"Recommendation algorithms should actively counteract historical digitization biases by promoting underrepresented collections."*
- *"The most valuable recommendations are those that reveal connections between collections that human curators would never have made."*

**System Stakeholders:**
- *"Platform sustainability requires recommendations that increase overall user engagement, even if this means some collections receive less visibility."*
- *"The system should optimize for the collective good of the platform rather than individual stakeholder preferences."*

**Consumer Stakeholders:**
- *"I prefer recommendations that confirm my existing research direction rather than those that challenge my assumptions."*
- *"The discovery of unexpected connections between materials is more valuable than finding additional sources that support my current thesis."*

**Downstream Stakeholders:**
- *"Recommendations should prioritize materials that have clear potential for public engagement over those with purely academic value."*
- *"Educational impact should take precedence over archival completeness when making recommendation decisions."*

---

### Topic 2: Adaptation and Access

**Key Question**: How should recommendations adapt to users with varying levels of expertise and different needs?

#### Interface Scenarios Presented:

**Scenario A: Expert-Oriented Interface**
- Recommendations presented with full scholarly apparatus and specialized terminology
- Assumes familiarity with domain-specific concepts and research methodologies
- Optimizes for information density and professional research workflows

**Scenario B: Accessibility-Oriented Interface**
- Recommendations presented with contextual explanations and progressive disclosure of complexity
- Provides scaffolding for users developing domain expertise
- Balances accessibility with content accuracy through layered information architecture

#### Discussion Questions (All Groups):
1. How should recommendations differ for users with different expertise levels, or should they differ at all?
2. How might user satisfaction be measured differently across expertise levels?
3. How would you measure successful knowledge transfer?

#### Provocative Statements by Stakeholder Group:

**Upstream Stakeholders:**
- *"Scholarly accuracy should never be sacrificed for accessibilityâ€”if users can't understand the technical terminology, they should consult reference materials rather than being shown simplified content."*
- *"Creating simplified versions of content for different expertise levels is an essential part of democratizing access to cultural heritage and should be a priority even if it requires additional resources."*

**Provider Stakeholders:**
- *"Adaptive interfaces are necessary, but the burden of creating multiple content versions should not fall entirely on content providers."*
- *"One-size-fits-all approaches fail everyone; we need sophisticated user modeling to serve different expertise levels effectively."*

**System Stakeholders:**
- *"The system should automatically detect user expertise level rather than relying on explicit user declarations."*
- *"User control over interface complexity is more important than algorithmic decisions about appropriate expertise levels."*

**Consumer Stakeholders:**
- *"A recommendation system that adapts to my expertise level might limit my growth by not exposing me to more advanced materials I should be learning about."*
- *"I prefer a system that meets me where I am and gradually increases complexity as I interact with the platform, rather than overwhelming me with expert-level content immediately."*

**Downstream Stakeholders:**
- *"Educational applications require clear learning pathways that recommendation algorithms can support better than human curation."*
- *"The most successful educational experiences come from progressive disclosure of complexity, which algorithms can manage more consistently than human instructors."*

---

### Topic 3: Transparency and Trust

**Key Question**: How should recommendations balance utility with transparency?

#### Interface Scenarios Presented:

**Scenario A: Algorithm-Centric Display**
- Recommendations shown with minimal explanation
- Emphasis on streamlined user experience
- Trust through performance rather than explanation

**Scenario B: Provenance-Centric Display**
- Recommendations shown with clear indication of document provenance and authority
- Traditional scholarly attribution approaches
- Trust through institutional authority

**Scenario C: Process-Centric Display**
- Recommendations shown with thorough explanation of why item is suggested
- Detailed algorithmic transparency and decision rationale
- Trust through understanding and control


#### Discussion Questions (All Groups):
1. How important is understanding why a document is recommended?
2. What level of transparency would build trust in recommendations?
3. How would you measure trustworthiness of the RecSys explanation?

#### Provocative Statements by Stakeholder Group:

**Upstream Stakeholders:**
- *"Curatorial decisions made by algorithms should be held to higher standards of transparency than those made by human archivists and curators."*
- *"Professional archival expertise should be explicitly acknowledged in recommendation explanations, not hidden behind algorithmic processes."*

**Provider Stakeholders:**
- *"Technical transparency is less important than demonstrable quality outcomes in building user trust."*
- *"Users should be able to audit recommendation decisions that affect the visibility of our collections."*

**System Stakeholders:**
- *"Too much transparency about algorithmic processes can lead to gaming and manipulation of the system."*
- *"User trust is built through consistent performance, not through detailed explanations that most users won't understand anyway."*

**Consumer Stakeholders:**
- *"I don't need to understand why something is recommended as long as it's relevant to my research."*
- *"I need to understand the basis on which sources are being recommended to me, regardless of how technical that explanation might be."*

**Downstream Stakeholders:**
- *"For educational applications, the process of understanding how recommendations work is as valuable as the recommendations themselves."*
- *"Students and educators need recommendation transparency for pedagogical credibility, not just research effectiveness."*

# Conclusion [ANONYMIZED]